{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f57b385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some reference material\n",
    "# 1) https://www.r-bloggers.com/2015/09/selecting-the-number-of-neurons-in-the-hidden-layer-of-a-neural-network/\n",
    "#    #:~:text=The%20most%20common%20rule%20of,number%20is%20greater%20than%201).\n",
    "# 2) https://www.analyticsvidhya.com/blog/2021/08/\n",
    "#    a-walk-through-of-regression-analysis-using-artificial-neural-networks-in-tensorflow/\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bc98ccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import feature_selection\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "def impute(df_train, df_test, method):\n",
    "    # There are  missing values in our data set lets replace them with the mean\n",
    "    \n",
    "    #df_train = df_train.fillna(df_train.mean())\n",
    "    #df_test = df_test.fillna(df_test.mean())\n",
    "    \n",
    "    if method == 'KNN':\n",
    "        imputer_train = KNNImputer(n_neighbors = 2)\n",
    "        df_train = imputer_train.fit_transform(df_train.copy())\n",
    "\n",
    "        imputer_test = KNNImputer(n_neighbors = 2)\n",
    "        df_test = imputer_test.fit_transform(df_test.copy())\n",
    "    \n",
    "    return pd.DataFrame(df_train), pd.DataFrame(df_test)\n",
    "    \n",
    "    #Outlier detection\n",
    "def filter(df1, df2):\n",
    "  # Filter feature selection\n",
    "  from sklearn.feature_selection import VarianceThreshold \n",
    "  \n",
    "  # Step 1: Removing Constant features\n",
    "  constant_filter = VarianceThreshold(threshold=0)\n",
    "  data_constant = constant_filter.fit_transform(df1)\n",
    "  #print(data_constant.shape)\n",
    "  constant_columns = [column for column in df1.columns if column not in df1.columns[constant_filter.get_support()]]\n",
    "  data_cons1 = df1.drop(constant_columns,axis=1)\n",
    "  data_cons2 = df2.drop(constant_columns,axis=1)\n",
    "      \n",
    "  # Step 2: Removing Quasi-Constant Features\n",
    "  qcons_filter = VarianceThreshold(threshold=0.001)\n",
    "  data_qcons = qcons_filter.fit_transform(df1)\n",
    "  #print(data_qcons.shape)\n",
    "  qcons_columns = [column for column in df1.columns if column not in df1.columns[qcons_filter.get_support()]]\n",
    "  data_qcons1 = df1.drop(qcons_columns,axis=1)\n",
    "  data_qcons2 = df2.drop(qcons_columns,axis=1)\n",
    "  data_qcons_t1 = data_qcons1.T\n",
    "  data_qcons_t2 = data_qcons2.T \n",
    "  \n",
    "  # Step 3: Removing Duplicate Columns\n",
    "  data_cons_dup1 = data_qcons_t1.drop_duplicates(keep='first').T\n",
    "  data_cons_dup2 = data_qcons_t2.drop_duplicates(keep='first').T\n",
    "\n",
    "  return data_cons_dup1, data_cons_dup2\n",
    "\n",
    "def reduce_feats(X):\n",
    "    ### performs PCA on the preprocessed features ###\n",
    "    pca = PCA(n_components=0.25, whiten=True)\n",
    "    return pca.fit_transform(X)\n",
    "\n",
    "def test_models(X_train_model, y_train_model, X_val, y_val):\n",
    "    bayes_ridge = BayesianRidge()\n",
    "    bayes_ridge2 = BayesianRidge(alpha_init = 1e-2, lambda_init = 1e-3)\n",
    "    linear = LinearRegression()\n",
    "    \n",
    "    models = [bayes_ridge, bayes_ridge2, linear]\n",
    "    scores = []\n",
    "    for model in models:\n",
    "        model.fit(X_train_model, y_train_model)\n",
    "        y_pred = model.predict(X_val)\n",
    "        scores.append(r2_score(y_val, y_pred))\n",
    "        \n",
    "    print(scores)    \n",
    "    return models, scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "368fd4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if any nan left: False False\n",
      "(1212, 23)\n",
      "[0.3393211103243102, 0.3393211098516762, 0.3375158904859149]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([BayesianRidge(),\n",
       "  BayesianRidge(alpha_init=0.01, lambda_init=0.001),\n",
       "  LinearRegression()],\n",
       " [0.3393211103243102, 0.3393211098516762, 0.3375158904859149])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = pd.read_csv(\"X_test.csv\", index_col=0).values\n",
    "X_train = pd.read_csv(\"X_train.csv\", index_col=0).values\n",
    "y_train = pd.read_csv(\"y_train.csv\", index_col=0).values\n",
    "\n",
    "df_train_orig = pd.DataFrame(X_train)\n",
    "df_test_orig = pd.DataFrame(X_test)\n",
    "\n",
    "# There are  missing values in our data set, let's carry out data imputation\n",
    "df_train, df_test = impute(df_train_orig, df_test_orig, 'KNN')\n",
    "print('Check if any nan left:', df_train.isnull().values.any(), df_test.isnull().values.any())\n",
    "\n",
    "# Apply std scaler and reduce dimensions \n",
    "X_train, X_test = filter(df_train, df_test)\n",
    "X_train, X_test = X_train.values, X_test.values\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "X_train = reduce_feats(X_train_scaled )\n",
    "#y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "# Split the data into training set and testing set\n",
    "X_train_model, X_val, y_train_model, y_val = train_test_split(X_train, y_train, \n",
    "                                                              test_size = 0.25, random_state = 42)\n",
    "y_train_model = np.ravel(y_train_model)\n",
    "y_val = np.ravel(y_val)\n",
    "\n",
    "# Test the models on the validation set we have from splitting the data\n",
    "test_models(X_train_model, y_train_model, X_val, y_val)\n",
    "\n",
    "# Based on the testing commented above, pick a model for the final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964ee9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
    "\n",
    "def build_nn():\n",
    "    #unit array containing the units for the first, second and third hidden layer\n",
    "    units = np.array([512, 256, 256])\n",
    "    nn_model = Sequential([Dense(units[0], kernel_initializer = 'random_normal', activation = 'relu'), Dropout(0.25) \n",
    "                        Dense(units[1], kernel_initializer = 'random_normal', activation = 'relu'), Dropout(0.25)\n",
    "                        Dense(units[2], kernel_initializer = 'random_normal', activation = 'relu'), \n",
    "                        Dense(1, acivation = 'linear')])\n",
    "    return nn_model\n",
    "\n",
    "regressor_nn = build_nn()\n",
    "regressor_nn.compile(loss = msle, optimizer=Adam(learning_rate=learning_rate), metrics=[msle])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "433ee618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check original unmodified True True\n",
      "(1212, 23)\n",
      "2\n",
      "[0.3170788661812989, 0.31707886505090377, 0.3130501520901099]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13324/78448115.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimpute2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train_orig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_test_orig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'KNN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[1;31m# Apply std scaler and reduce dimensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13324/78448115.py\u001b[0m in \u001b[0;36mimpute2\u001b[1;34m(df_train, df_test, method, neighbors)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'KNN'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mimputer_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKNNImputer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimputer_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mimputer_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKNNImputer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    697\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_knn.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             reduce_func=process_chunk)\n\u001b[1;32m--> 297\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m             \u001b[1;31m# process_chunk modifies X in place. No return value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_chunked\u001b[1;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[0;32m   1631\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreduce_func\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m             \u001b[0mchunk_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mD_chunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1633\u001b[1;33m             \u001b[0mD_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreduce_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1634\u001b[0m             \u001b[0m_check_chunk_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1635\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mD_chunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_knn.py\u001b[0m in \u001b[0;36mprocess_chunk\u001b[1;34m(dist_chunk, start)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m                 \u001b[1;31m# distances for samples that needed imputation for column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m                 dist_subset = (dist_chunk[dist_idx_map[receivers_idx] - start]\n\u001b[0m\u001b[0;32m    260\u001b[0m                                [:, potential_donors_idx])\n\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"TESTING KNN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    TEEEESTIIINGGG \"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import feature_selection\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "X_test = pd.read_csv(\"X_test.csv\", index_col=0).values\n",
    "X_train = pd.read_csv(\"X_train.csv\", index_col=0).values\n",
    "y_train = pd.read_csv(\"y_train.csv\", index_col=0).values\n",
    "\n",
    "df_train_orig = pd.DataFrame(X_train)\n",
    "df_test_orig = pd.DataFrame(X_test)\n",
    "\n",
    "\n",
    "def impute2(df_train, df_test, method, neighbors):\n",
    "    # There are  missing values in our data set lets replace them with the mean\n",
    "    if method == 'mean':\n",
    "        df_train = df_train.fillna(df_train.copy().mean())\n",
    "        df_test = df_test.fillna(df_test.copy().mean())\n",
    "    \n",
    "    elif method == 'KNN':\n",
    "        imputer_train = KNNImputer(n_neighbors = neighbors)\n",
    "        df_train = imputer_train.fit_transform(df_train.copy())\n",
    "\n",
    "        imputer_test = KNNImputer(n_neighbors = neighbors)\n",
    "        df_test = imputer_test.fit_transform(df_test.copy())\n",
    "        \n",
    "    print('Check original unmodified', df_train_orig.isnull().values.any(), df_test_orig.isnull().values.any())\n",
    "    \n",
    "    return pd.DataFrame(df_train), pd.DataFrame(df_test)\n",
    "\n",
    "\n",
    "neighbors = range(2, 50)\n",
    "scores = []\n",
    "for n in neighbors:\n",
    "\n",
    "    df_train, df_test = impute2(df_train_orig, df_test_orig, 'KNN', n)\n",
    "    # Apply std scaler and reduce dimensions \n",
    "    X_train, X_test = filter(df_train, df_test)\n",
    "    X_train, X_test = X_train.values, X_test.values\n",
    "\n",
    "    X_scaler = StandardScaler()\n",
    "    y_scaler = StandardScaler()\n",
    "\n",
    "    X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "    X_train = reduce_feats(X_train_scaled )\n",
    "    #y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "\n",
    "    print(X_train.shape)\n",
    "\n",
    "    # Split the data into training set and testing set\n",
    "    X_train_model, X_val, y_train_model, y_val = train_test_split(X_train, y_train, \n",
    "                                                                  test_size = 0.2, random_state = 42)\n",
    "    y_train_model = np.ravel(y_train_model)\n",
    "    y_val = np.ravel(y_val)\n",
    "    \n",
    "    print(n)\n",
    "    mods, rvals = test_models(X_train_model, y_train_model, X_val, y_val)\n",
    "    scores.append(rvals[0])\n",
    "    \n",
    "plt.plot(neighbors, np.array(scores))\n",
    "plt.show()\n",
    "\n",
    "### Note: 2 neighbors best for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a15bb9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check original unmodified True True\n",
      "Check new no nan False False\n",
      "(1212, 22)\n",
      "[0.31377220601580125, 0.31377220523913363, 0.3101474492547174]\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = impute2(df_train_orig, df_test_orig, 'mean', 1)\n",
    "print('Check new no nan', df_train.isnull().values.any(), df_test.isnull().values.any())\n",
    "X_train, X_test = filter(df_train, df_test)\n",
    "X_train, X_test = X_train.values, X_test.values\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "X_train = reduce_feats(X_train_scaled )\n",
    "#y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "# Split the data into training set and testing set\n",
    "X_train_model, X_val, y_train_model, y_val = train_test_split(X_train, y_train, \n",
    "                                                              test_size = 0.2, random_state = 42)\n",
    "y_train_model = np.ravel(y_train_model)\n",
    "y_val = np.ravel(y_val)\n",
    "\n",
    "mods, rvals = test_models(X_train_model, y_train_model, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e32f8330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.97805212620027\n",
      "0.011113821386102389\n"
     ]
    }
   ],
   "source": [
    "print(np.var(y_val))\n",
    "print(1/np.var(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b268ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1212, 773)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81ed88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "score = r2_score(y, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
