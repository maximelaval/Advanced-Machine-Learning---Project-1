{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fe918eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# There are  missing values in our data set lets replace them with the mean. \n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "print(len(X[X.isna().any(axis=1)]))\n",
    "print(len(y[y.isna().any(axis=1)])) # we also make sure there's no missing value for y . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc98ccdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1212, 252)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import feature_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Outlier detection\n",
    "def filter(df1, df2):\n",
    "  # Filter feature selection\n",
    "  from sklearn.feature_selection import VarianceThreshold \n",
    "  \n",
    "  # Step 1: Removing Constant features\n",
    "  constant_filter = VarianceThreshold(threshold=0)\n",
    "  data_constant = constant_filter.fit_transform(df1)\n",
    "  #print(data_constant.shape)\n",
    "  constant_columns = [column for column in df1.columns if column not in df1.columns[constant_filter.get_support()]]\n",
    "  data_cons1 = df1.drop(constant_columns,axis=1)\n",
    "  data_cons2 = df2.drop(constant_columns,axis=1)\n",
    "      \n",
    "  # Step 2: Removing Quasi-Constant Features\n",
    "  qcons_filter = VarianceThreshold(threshold=0.001)\n",
    "  data_qcons = qcons_filter.fit_transform(df1)\n",
    "  #print(data_qcons.shape)\n",
    "  qcons_columns = [column for column in df1.columns if column not in df1.columns[qcons_filter.get_support()]]\n",
    "  data_qcons1 = df1.drop(qcons_columns,axis=1)\n",
    "  data_qcons2 = df2.drop(qcons_columns,axis=1)\n",
    "  data_qcons_t1 = data_qcons1.T\n",
    "  data_qcons_t2 = data_qcons2.T \n",
    "  \n",
    "  # Step 3: Removing Duplicate Columns\n",
    "  data_cons_dup1 = data_qcons_t1.drop_duplicates(keep='first').T\n",
    "  data_cons_dup2 = data_qcons_t2.drop_duplicates(keep='first').T\n",
    "\n",
    "  return data_cons_dup1, data_cons_dup2\n",
    "\n",
    "def reduce_feats(X):\n",
    "    ### performs PCA on the preprocessed features ###\n",
    "    pca = PCA(n_components=0.75, whiten=True)\n",
    "    return pca.fit_transform(X)\n",
    "\n",
    "\n",
    "\n",
    "X_test = pd.read_csv(\"X_test.csv\", index_col=0).values\n",
    "X_train = pd.read_csv(\"X_train.csv\", index_col=0).values\n",
    "y_train = pd.read_csv(\"y_train.csv\", index_col=0).values\n",
    "\n",
    "df_train = pd.DataFrame(X_train)\n",
    "df_test = pd.DataFrame(X_test)\n",
    "\n",
    "# There are  missing values in our data set lets replace them with the mean. \n",
    "df_train = df_train.fillna(df_train.mean())\n",
    "df_test = df_test.fillna(df_test.mean())\n",
    "\n",
    "X_train, X_test = filter(df_train, df_test)\n",
    "X_train, X_test = X_train.values, X_test.values\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "# Apply std scaler and reduce dimensions \n",
    "X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "X_train = reduce_feats(X_train_scaled )\n",
    "#y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "# Split the data into training set and testing set\n",
    "X_train_model, X_val, y_train_model, y_val = train_test_split(X_train, y_train, \n",
    "                                                              test_size = 0.2, random_state = 42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6d58351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1212, 773)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4725bbab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
