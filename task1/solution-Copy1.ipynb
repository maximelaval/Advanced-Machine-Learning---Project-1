{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "076aec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import BayesianRidge,Lasso,LassoCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor,AdaBoostRegressor,GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor,StackingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import cross_val_score,train_test_split,GridSearchCV,KFold\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy import stats\n",
    "from sklearn import feature_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RationalQuadratic\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def split_data(X_train,y_train) :\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train,test_size = 0.25, random_state = 43)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def standardize_data(X_train,X_test) :\n",
    "    s = StandardScaler()\n",
    "    train_scaler = RobustScaler()\n",
    "    scaled_train_data = train_scaler.fit_transform(X_train)\n",
    "    scaled_test_data = train_scaler.transform(X_test)\n",
    "    \n",
    "    return scaled_train_data,scaled_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3cab2ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"X_test.csv\", index_col=0).values\n",
    "X_train = pd.read_csv(\"X_train.csv\", index_col=0).values\n",
    "y_train = pd.read_csv(\"y_train.csv\", index_col=0).values\n",
    "\n",
    "#X_train, X_test, y_train, y_test = split_data(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3fe66443",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define some imputers\n",
    "imputers = [\n",
    "    SimpleImputer(missing_values=np.nan, strategy='median'),\n",
    "    IterativeImputer(random_state=0, estimator=BayesianRidge()),\n",
    "    IterativeImputer(random_state=0, estimator=DecisionTreeRegressor(max_features=\"sqrt\", random_state=0)),\n",
    "    IterativeImputer(random_state=0, estimator=ExtraTreesRegressor(n_estimators=15, random_state=0, max_depth=7, min_samples_leaf=2)),\n",
    "    IterativeImputer(random_state=0, estimator=KNeighborsRegressor(n_neighbors=15)),\n",
    "    KNNImputer(n_neighbors=10, weights=\"uniform\"),\n",
    "    IterativeImputer(random_state=0, estimator=RandomForestRegressor(n_estimators= 35, random_state = 0, max_depth= 30, min_samples_leaf=2))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "902d10cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = imputation(imputers[0], X_train, X_test)\n",
    "\n",
    "X_train_0,X_test_0 = standardize_data(X_train,X_test)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d4859cb3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7117df0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def lasso(X_train,y_train,X_test) :\n",
    "    pipeline = Pipeline([('scaler',StandardScaler()),('model',Lasso())])\n",
    "    search = GridSearchCV(pipeline,{'model__alpha':np.arange(0.1,10,0.1)},cv = 5, scoring=\"r2\",verbose=3)\n",
    "    search.fit(X_train,y_train)\n",
    "    coefficients = search.best_estimator_.named_steps['model'].coef_\n",
    "    importance = np.abs(coefficients)\n",
    "    X_train = X_train[:,importance > 0]\n",
    "    X_test = X_test[:,importance > 0]\n",
    "    return X_train,X_test\n",
    "\n",
    "def lasso1(X_train,y_train,X_test) :\n",
    "    ls=LassoCV(cv=5)\n",
    "    ls.fit(X_train,y_train)\n",
    "    mask=ls.coef_!=0\n",
    "    X_train=X_train[:,mask]\n",
    "    X_test = X_test[:,mask]\n",
    "    \n",
    "\n",
    "def imputation(imputer, X_train, X_test):\n",
    "    imputer.fit(X_train)\n",
    "    X_train_0 = imputer.transform(X_train)\n",
    "    X_test_0 = imputer.transform(X_test)\n",
    "    return X_train_0, X_test_0\n",
    "\n",
    "def features_selection(X_train, y_train, X_test,  n_features):\n",
    "    from sklearn import feature_selection\n",
    "    model = feature_selection.SelectKBest(score_func=feature_selection.f_regression,k=n_features)\n",
    "    model = model.fit(X_train, y_train)\n",
    "    cols = model.get_support(indices=True)\n",
    "    \n",
    "    return cols\n",
    "\n",
    "def outlier_detection(X_train, y_train):\n",
    "    clf = IsolationForest(max_samples=100, random_state = 4)\n",
    "    preds = clf.fit_predict(X_train)\n",
    "    X_train_1 = X_train[preds==1]\n",
    "    y_train_1 = y_train[preds==1]\n",
    "    return X_train_1, y_train_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b0b108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGB():\n",
    "    def __init__(self, X_train, y_train, X_test):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.n_original_features = X_train.shape[1]\n",
    "        self.selected_features = np.arange(self.n_original_features)\n",
    "\n",
    "        #self.outlier_detection(self.selected_features, self.y_train)\n",
    "\n",
    "      \n",
    "        #Use stacking regressor\n",
    "        self.estimators = [('lasso', Lasso(alpha=0.0005, random_state =  0, max_iter=100)),\n",
    "                            ('xgb',XGBRegressor(max_depth=6,# depth of the tree\n",
    "                                    learning_rate=0.08,\n",
    "                                    n_estimators=100,# number of the tree\n",
    "                                    )),\n",
    "                           ('abr',AdaBoostRegressor(random_state=0, n_estimators=100)),\n",
    "                           ('dtr', DecisionTreeRegressor(max_features=\"sqrt\", random_state=0)), \n",
    "                           ('etr', ExtraTreesRegressor(n_estimators=15, random_state=0, max_depth=7, min_samples_leaf=2)),\n",
    "                           ('rfr', RandomForestRegressor(n_estimators= 15, random_state = 0, max_depth= 6, min_samples_leaf=2)),\n",
    "                           ('knr', KNeighborsRegressor(n_neighbors=15)), \n",
    "                           ('gbr', GradientBoostingRegressor(n_estimators = 100,learning_rate=0.05,\n",
    "                                                              max_depth = 10, random_state=0))]\n",
    "\n",
    "        \n",
    "        self.stacked_regressor = StackingRegressor(estimators=self.estimators)\n",
    "        \n",
    "        self.regressor = XGBRegressor(max_depth=17,\n",
    "n_estimators=115,\n",
    "learning_rate=0.07,\n",
    "subsample=0.9,\n",
    "colsample_bytree=0.6,\n",
    "min_child_weight=7,\n",
    "gamma=0.6000000000000001,\n",
    "reg_alpha=0.9,\n",
    "reg_lambda=0.7000000000000001)\n",
    "            \n",
    "\n",
    "        \n",
    "    def feature_selection(self, n_features = 200):\n",
    "        self.regressor.fit(self.X_train, self.y_train)\n",
    "        self.selected_features = np.argsort(self.regressor.feature_importances_)[::-1][:n_features]\n",
    "        return self.selected_features\n",
    "    \n",
    "    def feature_selection_stacked(self, n_features = 200):\n",
    "        self.stacked_regressor.fit(self.X_train, self.y_train)\n",
    "        self.selected_features = np.argsort(self.stacked_regressor.feature_importances_)[::-1][:n_features]\n",
    "        \n",
    "        return self.selected_features\n",
    "\n",
    "    def cross_validation(self, n_split = 8):\n",
    "        ret = cross_val_score(self.regressor, self.X_train[:,self.selected_features], self.y_train, scoring='r2', cv=n_split)\n",
    "        return ret\n",
    "\n",
    "    def predict(self, write2csv = True):\n",
    "        self.regressor.fit(self.X_train, self.y_train)\n",
    "        pred = self.regressor.predict(self.X_test)\n",
    "\n",
    "        if write2csv is True:\n",
    "            submission = np.hstack([np.arange(0, len(pred)).reshape(-1,1), pred.reshape(-1,1)]) \n",
    "            submission_pd = pd.DataFrame(submission, columns=['id','y'])\n",
    "            submission_pd.to_csv('submission.csv', index=None)\n",
    "\n",
    "        return pred\n",
    "    \n",
    "    def predict_stacked(self, write2csv = True):\n",
    "        self.stacked_regressor.fit(self.X_train[:,self.selected_features], self.y_train)\n",
    "        pred = self.stacked_regressor.predict(self.X_test[:,self.selected_features])\n",
    "        \n",
    "        if write2csv is True:\n",
    "            submission = np.hstack([np.arange(0, len(pred)).reshape(-1,1), pred.reshape(-1,1)]) \n",
    "            submission_pd = pd.DataFrame(submission, columns=['id','y'])\n",
    "            submission_pd.to_csv('submission.csv', index=None)\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def do_all(self, n_features = 200):\n",
    "        self.feature_selection(n_features)\n",
    "        pred = self.predict()\n",
    "        return pred\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e84cdca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maximelaval/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:302: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= X_norms\n"
     ]
    }
   ],
   "source": [
    "selected_features = features_selection(X_train_0, y_train.ravel(), X_test_0,n_features=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "959af5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>822</th>\n",
       "      <th>823</th>\n",
       "      <th>824</th>\n",
       "      <th>825</th>\n",
       "      <th>826</th>\n",
       "      <th>827</th>\n",
       "      <th>828</th>\n",
       "      <th>829</th>\n",
       "      <th>830</th>\n",
       "      <th>831</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.466690</td>\n",
       "      <td>-0.282618</td>\n",
       "      <td>-0.263665</td>\n",
       "      <td>-0.618864</td>\n",
       "      <td>-0.976612</td>\n",
       "      <td>-0.786602</td>\n",
       "      <td>1.195982</td>\n",
       "      <td>1.385506</td>\n",
       "      <td>1.227129</td>\n",
       "      <td>0.253626</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004431</td>\n",
       "      <td>-0.439093</td>\n",
       "      <td>0.203576</td>\n",
       "      <td>0.060724</td>\n",
       "      <td>-1.100703</td>\n",
       "      <td>-0.780246</td>\n",
       "      <td>-0.128824</td>\n",
       "      <td>-0.493355</td>\n",
       "      <td>-1.829907</td>\n",
       "      <td>-0.714606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.145548</td>\n",
       "      <td>-0.068001</td>\n",
       "      <td>1.574828</td>\n",
       "      <td>-0.755075</td>\n",
       "      <td>-0.013823</td>\n",
       "      <td>-0.010194</td>\n",
       "      <td>0.032176</td>\n",
       "      <td>1.236136</td>\n",
       "      <td>-1.506105</td>\n",
       "      <td>1.515109</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004431</td>\n",
       "      <td>1.406965</td>\n",
       "      <td>-0.199857</td>\n",
       "      <td>0.526490</td>\n",
       "      <td>-0.594855</td>\n",
       "      <td>-0.013314</td>\n",
       "      <td>-1.725024</td>\n",
       "      <td>0.008588</td>\n",
       "      <td>-1.409717</td>\n",
       "      <td>1.818198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.440705</td>\n",
       "      <td>0.057534</td>\n",
       "      <td>-0.063645</td>\n",
       "      <td>2.577001</td>\n",
       "      <td>-1.648303</td>\n",
       "      <td>-0.766725</td>\n",
       "      <td>0.007776</td>\n",
       "      <td>-0.015279</td>\n",
       "      <td>-0.005384</td>\n",
       "      <td>0.861130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281152</td>\n",
       "      <td>0.340408</td>\n",
       "      <td>-0.425105</td>\n",
       "      <td>-0.482133</td>\n",
       "      <td>1.632975</td>\n",
       "      <td>0.350198</td>\n",
       "      <td>0.471372</td>\n",
       "      <td>-1.850498</td>\n",
       "      <td>-0.125838</td>\n",
       "      <td>-0.439005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.894254</td>\n",
       "      <td>-2.349020</td>\n",
       "      <td>-3.001590</td>\n",
       "      <td>0.016241</td>\n",
       "      <td>-0.077519</td>\n",
       "      <td>0.214123</td>\n",
       "      <td>0.070541</td>\n",
       "      <td>0.011594</td>\n",
       "      <td>0.105183</td>\n",
       "      <td>0.112141</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065329</td>\n",
       "      <td>-3.386258</td>\n",
       "      <td>-2.778864</td>\n",
       "      <td>0.043016</td>\n",
       "      <td>-0.078849</td>\n",
       "      <td>-2.761680</td>\n",
       "      <td>0.030894</td>\n",
       "      <td>-1.695968</td>\n",
       "      <td>-0.141913</td>\n",
       "      <td>0.000119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.631951</td>\n",
       "      <td>1.536183</td>\n",
       "      <td>-0.047770</td>\n",
       "      <td>-0.852693</td>\n",
       "      <td>-0.476465</td>\n",
       "      <td>-0.449983</td>\n",
       "      <td>-0.336280</td>\n",
       "      <td>0.952266</td>\n",
       "      <td>0.441740</td>\n",
       "      <td>2.109532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029017</td>\n",
       "      <td>0.242487</td>\n",
       "      <td>-0.107779</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>-1.749368</td>\n",
       "      <td>0.424026</td>\n",
       "      <td>-0.085102</td>\n",
       "      <td>0.436115</td>\n",
       "      <td>0.671595</td>\n",
       "      <td>-0.573007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 832 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.466690 -0.282618 -0.263665 -0.618864 -0.976612 -0.786602  1.195982   \n",
       "1  1.145548 -0.068001  1.574828 -0.755075 -0.013823 -0.010194  0.032176   \n",
       "2 -0.440705  0.057534 -0.063645  2.577001 -1.648303 -0.766725  0.007776   \n",
       "3 -2.894254 -2.349020 -3.001590  0.016241 -0.077519  0.214123  0.070541   \n",
       "4 -0.631951  1.536183 -0.047770 -0.852693 -0.476465 -0.449983 -0.336280   \n",
       "\n",
       "        7         8         9    ...       822       823       824       825  \\\n",
       "0  1.385506  1.227129  0.253626  ... -0.004431 -0.439093  0.203576  0.060724   \n",
       "1  1.236136 -1.506105  1.515109  ... -0.004431  1.406965 -0.199857  0.526490   \n",
       "2 -0.015279 -0.005384  0.861130  ...  0.281152  0.340408 -0.425105 -0.482133   \n",
       "3  0.011594  0.105183  0.112141  ... -0.065329 -3.386258 -2.778864  0.043016   \n",
       "4  0.952266  0.441740  2.109532  ...  0.029017  0.242487 -0.107779  0.000324   \n",
       "\n",
       "        826       827       828       829       830       831  \n",
       "0 -1.100703 -0.780246 -0.128824 -0.493355 -1.829907 -0.714606  \n",
       "1 -0.594855 -0.013314 -1.725024  0.008588 -1.409717  1.818198  \n",
       "2  1.632975  0.350198  0.471372 -1.850498 -0.125838 -0.439005  \n",
       "3 -0.078849 -2.761680  0.030894 -1.695968 -0.141913  0.000119  \n",
       "4 -1.749368  0.424026 -0.085102  0.436115  0.671595 -0.573007  \n",
       "\n",
       "[5 rows x 832 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tr= pd.DataFrame(X_train_0)\n",
    "df_tr.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a43bb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  15  18  21  23  26  27  29  40  69  77  87  89  92  98 100 101 107\n",
      " 113 114 115 132 133 141 143 144 146 148 151 159 169 172 174 177 193 194\n",
      " 200 202 203 209 213 214 218 220 230 231 232 233 240 242 245 248 254 260\n",
      " 263 272 276 278 283 286 287 288 298 300 306 309 310 312 315 318 319 320\n",
      " 325 326 327 333 334 342 345 349 350 358 359 362 369 370 374 380 381 383\n",
      " 392 395 399 402 410 414 415 425 431 437 440 442 445 452 456 458 465 479\n",
      " 484 485 493 496 502 507 512 517 520 523 528 531 538 542 543 546 547 548\n",
      " 554 558 562 565 568 571 579 590 594 596 602 603 608 610 612 613 614 621\n",
      " 633 636 640 641 642 644 648 649 654 657 659 665 668 670 671 672 675 677\n",
      " 681 685 690 696 702 703 711 712 713 715 720 721 725 726 727 731 734 742\n",
      " 745 748 759 766 768 769 773 774 777 778 780 783 788 790 796 801 817 819\n",
      " 823 824]\n"
     ]
    }
   ],
   "source": [
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f143c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data imputation for X_train and X_test, then using SelectKbest to pick up the best 200 features\n",
    "\n",
    "new_train = X_train[:,selected_features]\n",
    "new_test = X_test[:,selected_features]\n",
    "\n",
    "new_train,new_test = standardize_data(new_train,new_test)\n",
    "\n",
    "X_train, X_test = imputation(imputers[5], new_train, new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb812f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdffd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test =lasso(X_train,y_train,X_test)\n",
    "\n",
    "X_train,y_train = outlier_detection(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342c4bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = X_train.shape[1]\n",
    "print(samples)\n",
    "\n",
    "#estimator 100, learning rate 0.1, max_depth=7\n",
    "xgb4 = XGB(X_train, y_train, X_test)\n",
    "#xgb4.feature_selection(n_features=samples)\n",
    "pred = xgb4.predict(write2csv=True)\n",
    "\n",
    "#print(r2_score(y_test.ravel(),pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c33f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from hyperopt import fmin, tpe, hp,space_eval,rand,Trials,partial,STATUS_OK\n",
    "\n",
    "def GBM(argsDict):\n",
    "    max_depth = argsDict[\"max_depth\"] + 5\n",
    "    n_estimators = argsDict['n_estimators'] * 5 + 70\n",
    "    learning_rate = argsDict[\"learning_rate\"] * 0.02 + 0.05\n",
    "    subsample = argsDict[\"subsample\"] * 0.1 + 0.6\n",
    "    colsample_bytree = argsDict[\"colsample_bytree\"] * 0.1 + 0.6\n",
    "    min_child_weight = argsDict[\"min_child_weight\"] + 1\n",
    "    gamma = argsDict[\"min_child_weight\"] * 0.1\n",
    "    reg_alpha = argsDict[\"reg_alpha\"] * 0.1\n",
    "    reg_lambda = argsDict[\"reg_lambda\"] * 0.1\n",
    "    print(\"max_depth:\" + str(max_depth))\n",
    "    print(\"n_estimators:\" + str(n_estimators))\n",
    "    print(\"learning_rate:\" + str(learning_rate))\n",
    "    print(\"subsample:\" + str(subsample))\n",
    "    print(\"colsample_bytree:\" + str(colsample_bytree))\n",
    "    print(\"min_child_weight:\" + str(min_child_weight))\n",
    "    print(\"gamma:\" + str(gamma))\n",
    "    print(\"reg_alpha:\" + str(reg_alpha))\n",
    "    print(\"reg_lambda:\" + str(reg_lambda))\n",
    "\n",
    "    gbm = XGBRegressor(learning_rate = learning_rate,\n",
    "                        n_estimators = n_estimators,\n",
    "                        max_depth = max_depth,\n",
    "                        min_child_weight = min_child_weight,\n",
    "                        subsample = subsample,\n",
    "                        colsample_bytree = colsample_bytree,\n",
    "                        gamma = gamma,\n",
    "                        reg_alpha = reg_alpha,\n",
    "                        reg_lambda = reg_lambda\n",
    "    )\n",
    "\n",
    "    metric = cross_val_score(gbm,X_train,y_train,cv=10,scoring=\"r2\") \n",
    "    print(str(metric) + '\\n')\n",
    "    metric_m= metric.mean()\n",
    "    return -metric_m\n",
    "\n",
    "space = {\"max_depth\":hp.randint(\"max_depth\",15),\n",
    "         \"n_estimators\":hp.randint(\"n_estimators\",10),  #[0,1,2,3,4,5] -> [50,]\n",
    "         \"learning_rate\":hp.randint(\"learning_rate\",6),  #[0,1,2,3,4,5] -> 0.05,0.06\n",
    "         \"subsample\":hp.randint(\"subsample\",5),#[0,1,2,3,4] -> [0.6,0.7,0.8,0.9,1.0]\n",
    "         \"colsample_bytree\":hp.randint(\"colsample_bytree\",5),#[0,1,2,3,4] -> [0.6,0.7,0.8,0.9,1.0]\n",
    "         \"min_child_weight\":hp.randint(\"min_child_weight\",7), #[0,1,2,3,4,5,6] -> +1\n",
    "         \"gamma\":hp.randint(\"gamma\", 7), # * 0.1\n",
    "         \"reg_alpha\":hp.randint(\"reg_alpha\", 30), # * 0.1\n",
    "         \"reg_lambda\":hp.randint(\"reg_lambda\", 30), # * 0.1\n",
    "        }\n",
    "algo = partial(tpe.suggest,n_startup_jobs=1)\n",
    "best = fmin(GBM,space,algo=algo,max_evals=200)\n",
    "\n",
    "print(best)\n",
    "print(GBM(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f387e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
